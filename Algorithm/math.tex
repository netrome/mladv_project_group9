\documentclass[10pt,a4paper]{article}
%\usepackage[ruled,norelsize]{algorithm2e}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\DeclareMathOperator*{\argmax}{arg\,max}
\title{Sparse Gaussian}
\begin{document}
\maketitle
\section{mathematical notations}
We first give brief description of mathematical notations will be used through out the project. 

The original data set will be denoted as $\mathcal{D}$ which consists of $N$ $d$-dimensional vectors $\pmb{X}=\lbrace\pmb{x}^{(i)}=(x_1,\dots,x_d)\,|\, i=1,\dots,N\rbrace$. Let the new input data be $\pmb{x}^{*}=(x^*_1,\dots,x^*_d)$. The pseudo input data set is denoted as $\bar{\mathcal{D}}$ consists of $\bar{\pmb{X}}=\lbrace\pmb{\bar{x}}^{(i)}=(x_1,\dots,x_d)\,|\,i=1,\dots,M\rbrace$. $\pmb{X}$ is paired with target $\pmb{Y}=(y^{(1)},\dots,y^{(N)})$, notice that $y^{(i)}$ are scalars. $\pmb{x}^*$ is paired with new target $y^*$. The underlining latent function is denoted as $\pmb{f}(\pmb{x})=\pmb{y}$ and the pseudo one is $\bar{\pmb{f}}$. A Gaussian distribution is denoted as $\mathcal{N}(\pmb{f}|\pmb{m},\pmb{V})$ with mean $\pmb{m}$ and variance $\pmb{V}$.
\section{sparse Gaussian process}
We first give a zero mean Gaussian prior over the underlining latent function: $p(\pmb{f}|\pmb{X})=\mathcal{N}(\pmb{f}|\pmb{0},\pmb{K}_N)$ where $\pmb{K}_N$ is our kernel matrix with elements given by, $[\pmb{K}_N]_{ij}\equiv K_{\pmb{x}^{(i)}\pmb{x}^{(j)}}=K(\pmb{x}^{(i)},\pmb{x}^{(j)})$: Notice that this is the case that we have same number of $\pmb{x}^{(i)},\pmb{x}^{(j)}$. In case of different sizes, we use $\pmb{K}_{NM}$, i.e. $N$ rows for the first input matrix, $M$ rows for the second input matrix.
\begin{align}
K(\pmb{x}^{(i)},\pmb{x}^{(j)})=c\exp [-\frac{1}{2}\sum_{k=1}^{D}b_k(x^{(i)}_k-x^{(j)}_k)^2], \quad\pmb{\theta}\equiv \lbrace c,\pmb{b}\rbrace,
\end{align}
where $\pmb{\theta}$ is the hyperparameters.
We provide noises to $\pmb{f}$ such that $p(\pmb{y}|\pmb{f})=\mathcal{N}(\pmb{y}|\pmb{f},\sigma^2\pmb{I})$. By integrating out the latent function we have the marginal likelihood
\begin{align}
p(\pmb{y}|\pmb{X},\pmb{\theta})=\mathcal{N}(\pmb{y}|\pmb{0},\pmb{K}_N+\sigma^2\pmb{I})
\end{align}
For prediction, the new input $\pmb{x}^*$ conditioning on the observed data and hyperparameters. Let write the joint probability first
\begin{align}
p(y^*,\pmb{y}|\pmb{x}^*,\mathcal{D},\pmb{\theta})=\mathcal{N}\left(\begin{pmatrix}
0\\0
\end{pmatrix},\begin{pmatrix}
K_{\pmb{x}^*\pmb{x}^*}+\sigma^2 & \pmb{K}_{\pmb{x}^*N}\\
\pmb{K}_{\pmb{x}^*N}^T & \pmb{K}_N+\sigma^2\pmb{I} 
\end{pmatrix}\right),
\label{cgjoint}
\end{align}
where $\pmb{K}_{\pmb{x}^*N}=(K(\pmb{x}^*,\pmb{x}^{(1)}),\dots,K(\pmb{x}^*,\pmb{x}^{(N)}))$, i.e. $[\pmb{K}_{\pmb{x}^*N}]_{i}=K(\pmb{x}^*,\pmb{x}^{(i)})$, and $K_{\pmb{x}^*\pmb{x}^*}=K(\pmb{x}^*,\pmb{x}^*)$. 
Now we can condition on $\pmb{y}$ and get
\begin{align}
&p(y^*|\pmb{y},\pmb{x}^*,\mathcal{D},\pmb{\theta})\nonumber\\
&=\mathcal{N}(y^*|\pmb{K}_{\pmb{x}^*N}(\pmb{K}_N+\sigma^2\pmb{I})^{-1}\pmb{y},K_{\pmb{x}^*\pmb{x}^*}+\sigma^2-\pmb{K}_{\pmb{x}^*N}(\pmb{K}_N+\sigma^2\pmb{I})^{-1}\pmb{K}_{\pmb{x}^*N}^T).
\label{cgcondition}
\end{align}
For detailed proof, check Theorem 4.3.1 in Murphy's machine learning a probabilistic perspective.

Now we consider pseudo input $\bar{\pmb{X}}$. Everything still holds except that there are no noises in it. The new input and target pair $(\pmb{x}^*,y^*)$ is replaced by one of the actually data set and targets pairs $(\pmb{x}^{(i)},y_i)$. We therefore just use $\bar{\pmb{f}}$ represents the pseudo outputs and $\bar{\pmb{\theta}}$, and the single point likelihood is given by
\begin{align}
p(y|\pmb{x},\bar{\pmb{f}},\bar{\pmb{X}})=\mathcal{N}(y|\pmb{K}_{\pmb{x}M}\pmb{K}_M^{-1}\bar{\pmb{f}},K_{\pmb{x}\pmb{x}}+\sigma^2-\pmb{K}_{\pmb{x}M}\pmb{K}_M^{-1}\pmb{K}_{\pmb{x}M}^T),
\end{align}
where $\pmb{K}_{\pmb{x}M}=(K(\pmb{x},\bar{\pmb{x}}^{(1)}),\dots,K(\pmb{x},\bar{\pmb{x}}^{(M)}))$, i.e. $[\pmb{K}_{\pmb{x}M}]_{i}=K(\pmb{x},\bar{\pmb{x}}^{(i)})$.
As the target data are i.i.d given the inputs, the complete data likelihood is given by
\begin{align}
p(\pmb{y}|\pmb{X},\bar{\pmb{f}},\bar{\pmb{X}})=\prod^N_{i=1}p(y_i|\pmb{x}^{(i)},\bar{\pmb{f}},\bar{\pmb{X}})=\mathcal{N}(\pmb{y}|\pmb{K}_{NM}\pmb{K}_M^{-1}\bar{\pmb{f}}, \pmb{\Lambda}+\sigma^2\pmb{I}),
\label{cdlikelihood}
\end{align}
where $\pmb{\Lambda}=\text{diag}(\pmb{\lambda}),\lambda_i = K_{\pmb{x}^{(i)}\pmb{x}^{(i)}}-\pmb{K}_{\pmb{x}^{(i)}M}\pmb{K}_M^{-1}\pmb{K}_{\pmb{x}^{(i)}M}^T$, is a $N\times N$ diagonal matrix, and $[\pmb{K}_{NM}]_{ij}=K(\pmb{x}^{(i)},\pmb{x}^{(j)})$.
Together with a Gaussian prior, $p(\bar{\pmb{f}}|\bar{\pmb{X}})=\mathcal{N}(\bar{\pmb{f}}|\pmb{0},\pmb{K}_M)$, integrate over Eq.\ref{cdlikelihood} we have the SPGP marginal likelihood over pseudo inputs
\begin{align}
p(\pmb{y}|\pmb{X},\bar{\pmb{X}})&=\int p(\pmb{y}|\pmb{X},\bar{\pmb{f}},\bar{\pmb{X}})p(\bar{\pmb{f}}|\bar{\pmb{X}})\,\mathrm{d}\bar{\pmb{f}}\nonumber\\
&=\mathcal{N}(\pmb{y}|\pmb{0},\pmb{K}_{NM}\pmb{K}_M^{-1}\pmb{K}_{MN}+\pmb{\Lambda}+\sigma^2\pmb{I}).
\label{spgpmarginallikelihood}
\end{align}
Same as we have done from Eq.\ref{cgjoint} to Eq.\ref{cgcondition}, we first write the joint probability of $y^*,\pmb{y}$
\begin{align}
&p(y^*,\pmb{y}|\pmb{x}^*,\pmb{X},\bar{\pmb{X}})\\\nonumber
&=\mathcal{N}\left(\begin{pmatrix}
0\\0
\end{pmatrix},\begin{pmatrix}
 K_{\pmb{x}^*\pmb{x}^*}+\sigma^2 & \pmb{K}_{\pmb{x}^*M}\pmb{K}_M^{-1}\pmb{K}_{MN}\\
(\pmb{K}_{\pmb{x}^*M}\pmb{K}_M^{-1}\pmb{K}_{MN})^T & \pmb{K}_{NM}\pmb{K}_M^{-1}\pmb{K}_{MN}+\pmb{\Lambda}+\sigma^2\pmb{I}
\end{pmatrix}\right),
\end{align}
where $\pmb{K}_{\pmb{x}^*M}=(K(\pmb{x}^*,\bar{\pmb{x}}^{(1)}),\dots,K(\pmb{x}^*,\bar{\pmb{x}}^{(M)}))$, i.e. $[\pmb{K}_{\pmb{x}^*M}]_{i}=K(\pmb{x}^*,\bar{\pmb{x}}^{(i)})$. From now on we let 
\begin{align}
\pmb{Q}_{\pmb{X},\pmb{X}'}&\equiv\pmb{Q}(\pmb{X},\pmb{X}')=\pmb{K}_{\pmb{X}M}\pmb{K}_{M}^{-1}\pmb{K}_{M\pmb{X}'}\\
\pmb{Q}_N&=\pmb{K}_{NM}\pmb{K}_{M}^{-1}\pmb{K}_{MN},
\end{align}
Also, remember that here $N$ and $M$ represents input and pseudo input data set, matrices, as input matrices of $\pmb{K}$, respectively.
And after conditioning on $\pmb{y}$, we have the SPGP predictive distribution
\begin{align}
p(y^*|\pmb{y},\pmb{x}^*,\pmb{X},\bar{\pmb{X}})=\mathcal{N}(\mu^*,{\sigma^*}^2)
\end{align}
\begin{equation}
\begin{aligned}
\mu^*&=\pmb{Q}_{\pmb{x}^*N}(\pmb{Q}_N+\pmb{\Lambda}+\sigma^2\pmb{I})^{-1}\pmb{y}\\
{\sigma^*}^2&=K_{\pmb{x}^*\pmb{x}^*}-\pmb{Q}_{\pmb{x}^*N}(\pmb{Q}_N+\pmb{\Lambda}+\sigma^2\pmb{I})^{-1}\pmb{Q}_{N\pmb{x}^*}+\sigma^2.
\end{aligned}
\label{spgppredict}
\end{equation}
The pseudo input $\bar{\pmb{C}}$ and hyperparameters $\pmb{\Theta}=\lbrace\pmb{\theta},\sigma^2\rbrace$, this can be done by maximizing Eq.\ref{spgpmarginallikelihood}.

Some simplification for matrix inversion. First from matrix inversion lemma
\begin{align}
(\pmb{A}+\pmb{U}\pmb{B}\pmb{U}^T)^{-1}=\pmb{A}^{-1}-\pmb{A}^{-1}\pmb{U}(\pmb{B}^{-1}+\pmb{U}^T\pmb{A}^{-1}\pmb{U})^{-1}\pmb{U}^T\pmb{A}^{-1}\\
\det(\pmb{A}+\pmb{U}\pmb{B}\pmb{U}^T)=\det(\pmb{A})\det(\pmb{B})\det(\pmb{B}^{-1}+\pmb{U}^T\pmb{A}^{-1}\pmb{U}),
\end{align}
we can rewrite following
\begin{align}
&(\pmb{K}_{NM}\pmb{K}_{M}^{-1}\pmb{K}_{MN}+\pmb{\Lambda}+\sigma^2\pmb{I})^{-1}\\
&=(\pmb{\Lambda}+\sigma^2\pmb{I})^{-1}-(\pmb{\Lambda}+\sigma^2\pmb{I})^{-1}\pmb{K}_{NM}\pmb{B}^{-1}\pmb{K}_{MN}(\pmb{\Lambda}+\sigma^2\pmb{I})^{-1},
\end{align}
where $\pmb{B}=\pmb{K}_M+\pmb{K}_{MN}(\pmb{\Lambda}+\sigma^2\pmb{I})^{-1}\pmb{K}_{NM}$. Now matrix inversion only happens to $(\pmb{\Lambda}+\sigma^2\pmb{I})^{-1}$ which is $\mathcal{O}(N)$ as it is diagonal. Now Eq.\ref{spgppredict} become
\begin{equation}
\begin{aligned}
\mu^*&=\pmb{K}_{\pmb{x}^*M}\pmb{B}^{-1}\pmb{K}_{MN}(\pmb{\Lambda}+\sigma^2\pmb{I})^{-1}\pmb{y}\\
{\sigma^*}^2&=K_{\pmb{x}^*\pmb{x}^*}-\pmb{K}_{\pmb{x}^*M}(\pmb{K}_M^{-1}-\pmb{B}^{-1})\pmb{K}_{M\pmb{x}^*}+\sigma^2.
\end{aligned}
\label{spgppredictsim}
\end{equation}
\section{implementation}
Rewrite
\begin{align}
\sigma^2\Gamma=\Lambda+
\end{align}















\end{document}

